{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Evaluation Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and\n",
    "salary filter.\n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is ‚ÄúDelhi/NCR‚Äù. The salary filter to be used is ‚Äú3-6‚Äù lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "location.send_keys('Delhi / NCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryfilter=driver.find_element(By.XPATH,\"/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[5]/div[2]/div[2]/label/p/span[1]\")\n",
    "salaryfilter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Job Title  \\\n",
      "0                     Data Scientist HTHD   \n",
      "1                          Data Scientist   \n",
      "2                          Data Scientist   \n",
      "3                       Data Scientist II   \n",
      "4                     Lead Data Scientist   \n",
      "5                          Data Scientist   \n",
      "6  Lead Customer Success - Data Scientist   \n",
      "7                Fullstack Data Scientist   \n",
      "8                          Data Scientist   \n",
      "9                          Data Scientist   \n",
      "\n",
      "                                            Location               Company  \\\n",
      "0  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...                  Ford   \n",
      "1                        Mumbai, Hyderabad, Gurugram              Deloitte   \n",
      "2                                Gurugram, Bengaluru             Blackbuck   \n",
      "3                           Greater Noida, Bengaluru             Honeywell   \n",
      "4  Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...           Elitefit.ai   \n",
      "5  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...     Fort Technologies   \n",
      "6                                     Pune, Gurugram         ZS Associates   \n",
      "7                        Mumbai, Gurugram, Bengaluru  Course5 Intelligence   \n",
      "8                                              Noida            Innovaccer   \n",
      "9                                              Noida        Times Internet   \n",
      "\n",
      "  Experience Required  \n",
      "0             1-4 Yrs  \n",
      "1             3-6 Yrs  \n",
      "2             3-7 Yrs  \n",
      "3             3-6 Yrs  \n",
      "4             3-7 Yrs  \n",
      "5             1-3 Yrs  \n",
      "6             2-4 Yrs  \n",
      "7             3-6 Yrs  \n",
      "8             2-7 Yrs  \n",
      "9             3-8 Yrs  \n"
     ]
    }
   ],
   "source": [
    "job_titles=[]\n",
    "job_locations=[]\n",
    "companies=[]\n",
    "experiences=[]\n",
    "title_tags=driver.find_elements(By.XPATH,'//div[@class=\"cust-job-tuple layout-wrapper lay-2 sjw__tuple \"]/div/a')\n",
    "for i in title_tags:\n",
    "    t=i.text.strip()\n",
    "    job_titles.append(t)\n",
    "locs_tags=driver.find_elements(By.XPATH,'//span[@class=\"locWdth\"]')\n",
    "for i in locs_tags:\n",
    "    l=i.text.strip()\n",
    "    job_locations.append(l)\n",
    "company_tags=driver.find_elements(By.XPATH,'//div[@class=\" row2\"]/span/a[1]')\n",
    "for i in company_tags:\n",
    "    c=i.text.strip()\n",
    "    companies.append(c)\n",
    "exp_tags=driver.find_elements(By.XPATH,'//span[@class=\"expwdth\"]')\n",
    "for i in exp_tags:\n",
    "    e=i.text.strip()\n",
    "    experiences.append(e)\n",
    "df=pd.DataFrame({'Job Title':job_titles[:10],'Location':job_locations[:10],'Company':companies[:10],'Experience Required':experiences[:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the\n",
    "job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.shine.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"form-control\")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/input\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[2]/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Job Title        Location  \\\n",
      "0         Data Scientist Opening  Bangalore\\n+15   \n",
      "1         Data Scientist Opening  Bangalore\\n+15   \n",
      "2     Data Scientist Recruitment   Bangalore\\n+9   \n",
      "3                 Data Scientist   Bangalore\\n+8   \n",
      "4              ML Data Scientist   Bangalore\\n+3   \n",
      "5  Data Scientist Urgent Vacancy  Bangalore\\n+15   \n",
      "6                 Data Scientist   Bangalore\\n+4   \n",
      "7     Data Scientist Recruitment  Bangalore\\n+15   \n",
      "8     Data Scientist Recruitment  Bangalore\\n+15   \n",
      "9        Clinical Data Associate   Bangalore\\n+6   \n",
      "\n",
      "                                  Company Experience required  \n",
      "0                      renuka interprises          0 to 4 Yrs  \n",
      "1                      renuka interprises          0 to 4 Yrs  \n",
      "2                     radhika enterprises          0 to 4 Yrs  \n",
      "3                           techno endura          0 to 2 Yrs  \n",
      "4  gujarat facility services hiring fo...          5 to 8 Yrs  \n",
      "5                      renuka interprises          0 to 4 Yrs  \n",
      "6           acme services private limited          3 to 5 Yrs  \n",
      "7                      renuka interprises          0 to 4 Yrs  \n",
      "8                      renuka interprises          0 to 4 Yrs  \n",
      "9                           techno endura           0 to 1 Yr  \n"
     ]
    }
   ],
   "source": [
    "job_titles=[]\n",
    "job_locations=[]\n",
    "companies=[]\n",
    "experiences=[]\n",
    "location_tags=driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "for i in location_tags:\n",
    "    l=i.text.strip()\n",
    "    job_locations.append(l)\n",
    "company_tags=driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]')\n",
    "for i in company_tags:\n",
    "    c=i.text.strip()\n",
    "    companies.append(c)\n",
    "exp_tags=driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]')\n",
    "for i in exp_tags:\n",
    "    e=i.text.strip()\n",
    "    experiences.append(e)\n",
    "title_tags=driver.find_elements(By.XPATH,'//h2[@itemprop=\"name\"]')\n",
    "for i in title_tags:\n",
    "    t=i.text.strip()\n",
    "    job_titles.append(t)\n",
    "df=pd.DataFrame({'Job Title':job_titles[:10],'Location':job_locations[:10],'Company':companies[:10],'Experience required':experiences[:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-white-128-gb/product-reviews/itme32df47ea6742?pid=MOBFWQ6B7KKRXDDS&lid=LSTMOBFWQ6B7KKRXDDSUPVRTR&marketplace=FLIPKART\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=[]\n",
    "review_sums=[]\n",
    "reviews=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    \n",
    "    rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rating_tags:\n",
    "        rt=i.text.strip()\n",
    "        ratings.append(rt)\n",
    "    \n",
    "    reviewsum_tags=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in reviewsum_tags:\n",
    "        rs=i.text.strip()\n",
    "        review_sums.append(rs)\n",
    "    \n",
    "    review_tags=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in review_tags:\n",
    "        r=i.text.strip()\n",
    "        reviews.append(r)\n",
    "    \n",
    "    next_button=driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "    next_button.click()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Review summary Rating  \\\n",
      "0     Worth every penny      5   \n",
      "1   Best in the market!      5   \n",
      "2             Wonderful      5   \n",
      "3             Fabulous!      5   \n",
      "4              Terrific      5   \n",
      "..                  ...    ...   \n",
      "95               Super!      5   \n",
      "96            Fabulous!      5   \n",
      "97    Terrific purchase      5   \n",
      "98            Must buy!      5   \n",
      "99   Highly recommended      5   \n",
      "\n",
      "                                          Full Review  \n",
      "0   Feeling awesome after getting the delivery of ...  \n",
      "1                                         Good Camera  \n",
      "2                              This is amazing at all  \n",
      "3                     Superüî• and good performance üëå‚ù§Ô∏è  \n",
      "4                                      Very very good  \n",
      "..                                                ...  \n",
      "95                        Good product üëåI love iPhone  \n",
      "96  It‚Äôs very good battery life and display and vi...  \n",
      "97                                 Value for money üñ§üñ§  \n",
      "98  Go for iPhone 11 , if confused between iPhone ...  \n",
      "99  Awesome Battery Life...Camera clarity is too g...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame({'Review summary':review_sums,'Rating':ratings,'Full Review':reviews})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for ‚Äúsneakers‚Äù in the search\n",
    "field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/div/input\")\n",
    "designation.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands=[]\n",
    "prod_desc=[]\n",
    "prices=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    \n",
    "    brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brand_tags:\n",
    "        b=i.text.strip()\n",
    "        brands.append(b)\n",
    "    \n",
    "    prod_tags=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in prod_tags:\n",
    "        pds=i.text.strip()\n",
    "        prod_desc.append(pds)\n",
    "    \n",
    "    price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in price_tags:\n",
    "        p=i.text.strip()\n",
    "        prices.append(p)\n",
    "    \n",
    "    next_button=driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Brand                                Product Description   Price\n",
      "0      HOTSTYLE      Combo Pack Of 2 Casual Shoes Sneakers For Men    ‚Çπ294\n",
      "1        BRUTON  Trending Stylish Casual Outdoor Shoes For Men ...    ‚Çπ499\n",
      "2      URBANBOX                                 Sneakers For Women    ‚Çπ299\n",
      "3     Deals4you  Casual Sneakers Shoes for Men | Soft Cushioned...    ‚Çπ439\n",
      "4      RED TAPE                       Fire Run V1 Sneakers For Men  ‚Çπ1,179\n",
      "..          ...                                                ...     ...\n",
      "96       BRUTON  Sneakers For Mens/Sandals for mens Slip On Loa...  ‚Çπ1,308\n",
      "97         ATOM              Canvas shoes for Men Sneakers For Men    ‚Çπ629\n",
      "98   HIGHLANDER  Stylish Sneakers Shoes for Women And Girls Sne...  ‚Çπ1,408\n",
      "99     BERSACHE                                   Sneakers For Men    ‚Çπ374\n",
      "100        ATOM                     Shuffle Ultra Sneakers For Men    ‚Çπ549\n",
      "\n",
      "[101 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame({'Brand':brands[:101],'Product Description':prod_desc[:101],'Price':prices[:101]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Go to webpage https://www.amazon.in/ Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon. Then set CPU\n",
    "Type filter to ‚ÄúIntel Core i7‚Äù.\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "designation.send_keys('Laptops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_filter=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[19]/span/span[9]/li/span/a/div/label/i\")\n",
    "cpu_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title              Rating  \\\n",
      "0   Lenovo IdeaPad Slim 3 Intel Core i7 12th Gen 1...  4.1 out of 5 stars   \n",
      "1   MSI Modern 14, Intel 12th Gen. i7-1255U, 36CM ...  4.1 out of 5 stars   \n",
      "2   MSI Modern 14, Intel 12th Gen. i7-1255U, 36CM ...  4.1 out of 5 stars   \n",
      "3   Acer Travelmate Business Laptop Intel Core i7-...  5.0 out of 5 stars   \n",
      "4   ASUS TUF Gaming F15, 15.6\"(39.62 cms) FHD 144H...  4.3 out of 5 stars   \n",
      "5   Lenovo Yoga Slim7 Carbon Intel Evo i7 1260P 13...  3.1 out of 5 stars   \n",
      "6   HP Pavilion X360 11th Gen Intel Core i7 14\" (3...  3.9 out of 5 stars   \n",
      "7   ASUS Vivobook 15, Intel Core i7-12650H 12th Ge...  3.9 out of 5 stars   \n",
      "8   HP Laptop 15s, 12th Gen Intel Core i7-1255U, 1...  3.3 out of 5 stars   \n",
      "9   HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...  4.2 out of 5 stars   \n",
      "10  Lenovo IdeaPad Slim 3 Intel Core i7 11th Gen 1...  4.1 out of 5 stars   \n",
      "\n",
      "          Price  \n",
      "0     Rs 69,990  \n",
      "1     Rs 62,990  \n",
      "2     Rs 49,990  \n",
      "3     Rs 49,990  \n",
      "4     Rs 49,990  \n",
      "5     Rs 74,990  \n",
      "6   Rs 1,19,990  \n",
      "7     Rs 66,990  \n",
      "8     Rs 59,990  \n",
      "9     Rs 62,490  \n",
      "10    Rs 77,990  \n"
     ]
    }
   ],
   "source": [
    "titles=[]\n",
    "ratings=[]\n",
    "prices=[]\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_tags:\n",
    "    t=i.text.strip()\n",
    "    titles.append(t)\n",
    "price_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in price_tags:\n",
    "    p=i.text.strip()\n",
    "    prices.append('Rs '+p)\n",
    "rating_tags = driver.find_elements(By.XPATH,'//span[@class=\"a-icon-alt\"]')\n",
    "for i in rating_tags:\n",
    "    r=i.get_attribute(\"innerHTML\")\n",
    "    ratings.append(r)\n",
    "\n",
    "df=pd.DataFrame({'Title':titles[:11],'Rating':ratings[:11],'Price':prices[:11]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpage https://www.azquotes.com/\n",
    "2. Click on TopQuote\n",
    "3. Then scrap a)Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.azquotes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a\")\n",
    "destination.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Quote                Author  \\\n",
      "0   The essence of strategy is choosing what not t...        Michael Porter   \n",
      "1   One cannot and must not try to erase the past ...            Golda Meir   \n",
      "2   Patriotism means to stand by the country. It d...    Theodore Roosevelt   \n",
      "3   Death is something inevitable. When a man has ...        Nelson Mandela   \n",
      "4   You have to love a nation that celebrates its ...          Erma Bombeck   \n",
      "..                                                ...                   ...   \n",
      "95     When the going gets weird, the weird turn pro.    Hunter S. Thompson   \n",
      "96  When a train goes through a tunnel and it gets...       Corrie Ten Boom   \n",
      "97  If you think you are too small to make a diffe...            Dalai Lama   \n",
      "98  God doesn't require us to succeed, he only req...         Mother Teresa   \n",
      "99    Change your thoughts and you change your world.  Norman Vincent Peale   \n",
      "\n",
      "                               Type of Quote  \n",
      "0   Essence, Deep Thought, Transcendentalism  \n",
      "1                  Inspiration, Past, Trying  \n",
      "2                        Country, Peace, War  \n",
      "3         Inspirational, Motivational, Death  \n",
      "4               4th Of July, Food, Patriotic  \n",
      "..                                       ...  \n",
      "95                    Music, Sports, Hunting  \n",
      "96             Trust, Encouraging, Uplifting  \n",
      "97              Inspirational, Funny, Change  \n",
      "98                      Success, God, Mother  \n",
      "99       Inspirational, Motivational, Change  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "quotes=[]\n",
    "authors=[]\n",
    "types=[]\n",
    "quote_tags=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags:\n",
    "    q=i.text.strip()\n",
    "    quotes.append(q)\n",
    "author_tags=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags:\n",
    "    a=i.text.strip()\n",
    "    authors.append(a)\n",
    "type_tags=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in type_tags:\n",
    "    t=i.text.strip()\n",
    "    types.append(t)\n",
    "df=pd.DataFrame({'Quote':quotes,'Author':authors,'Type of Quote':types})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
